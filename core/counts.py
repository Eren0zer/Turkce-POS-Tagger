"""
HMM N-gram Counts ve Probability Calculator

Bu modÃ¼l Hidden Markov Model iÃ§in gerekli geÃ§iÅŸ ve emisyon olasÄ±lÄ±klarÄ±nÄ± hesaplar.
Smoothing teknikleri ve OOV (Out-of-Vocabulary) handling iÃ§erir.
"""

from collections import defaultdict, Counter
import math


class HMMCounts:
    """
    HMM iÃ§in n-gram sayÄ±mlarÄ±nÄ± tutan ve olasÄ±lÄ±k hesaplayan sÄ±nÄ±f.
    
    Bu sÄ±nÄ±f ÅŸu olasÄ±lÄ±klarÄ± hesaplar:
    - Transition probabilities: P(tag_i | tag_{i-1})
    - Emission probabilities: P(word | tag)
    """
    
    def __init__(self):
        """
        HMMCounts constructor. TÃ¼m sayaÃ§larÄ± initialize eder.
        """
        # P(tag_i | tag_{i-1}) iÃ§in sayÄ±mlar
        self.transition_counts = defaultdict(Counter)
        
        # P(word | tag) iÃ§in sayÄ±mlar
        self.emission_counts = defaultdict(Counter)
        
        # Her tag'in toplam sayÄ±sÄ±
        self.tag_counts = Counter()
        
        # Vocabulary (tÃ¼m unique kelimeler)
        self.vocab = set()
        
        # Ä°statistikler
        self.total_tokens = 0
        self.total_sentences = 0
    
    def count_from_sentences(self, sentences):
        """
        CÃ¼mlelerden n-gram sayÄ±mlarÄ±nÄ± Ã§Ä±karÄ±r.
        
        Args:
            sentences (list): Her cÃ¼mle iÃ§in token listesi iÃ§eren liste.
                            Token format: {'form': str, 'upos': str}
        
        Example:
            >>> counts = HMMCounts()
            >>> sentences = [[{'form': 'Bu', 'upos': 'PRON'}, {'form': 'kitap', 'upos': 'NOUN'}]]
            >>> counts.count_from_sentences(sentences)
        """
        print("ğŸ“Š N-gram sayÄ±mlarÄ± hesaplanÄ±yor...")
        
        for sentence in sentences:
            if not sentence:  # BoÅŸ cÃ¼mleleri atla
                continue
                
            self.total_sentences += 1
            
            # CÃ¼mle baÅŸlangÄ±cÄ± iÃ§in Ã¶zel tag
            prev_tag = "<START>"
            self.tag_counts[prev_tag] += 1
            
            for token in sentence:
                word = token['form'].lower()  # Kelimeleri kÃ¼Ã§Ã¼k harfe Ã§evir
                tag = token['upos']
                
                # SayÄ±mlarÄ± gÃ¼ncelle
                self.transition_counts[prev_tag][tag] += 1
                self.emission_counts[tag][word] += 1
                self.tag_counts[tag] += 1
                self.vocab.add(word)
                
                self.total_tokens += 1
                prev_tag = tag
            
            # CÃ¼mle sonu iÃ§in Ã¶zel tag
            self.transition_counts[prev_tag]["<END>"] += 1
        
        self._print_count_statistics()
    
    def get_transition_prob(self, prev_tag, curr_tag, smoothing=0.1):
        """
        Ä°ki tag arasÄ±ndaki geÃ§iÅŸ olasÄ±lÄ±ÄŸÄ±nÄ± hesaplar.
        
        P(curr_tag | prev_tag) = (count(prev_tag, curr_tag) + smoothing) / 
                                 (count(prev_tag) + smoothing * |tagset|)
        
        Args:
            prev_tag (str): Ã–nceki tag
            curr_tag (str): Åimdiki tag
            smoothing (float): Add-k smoothing parametresi
        
        Returns:
            float: Log olasÄ±lÄ±k deÄŸeri
        """
        numerator = self.transition_counts[prev_tag][curr_tag] + smoothing
        denominator = sum(self.transition_counts[prev_tag].values()) + smoothing * len(self.tag_counts)
        
        # SÄ±fÄ±r bÃ¶lme kontrolÃ¼
        if denominator == 0:
            return float('-inf')
        
        return math.log(numerator / denominator)
    
    def get_emission_prob(self, tag, word, smoothing=0.1):
        """
        Tag'den kelime Ã§Ä±kÄ±ÅŸ olasÄ±lÄ±ÄŸÄ±nÄ± hesaplar.
        
        P(word | tag) = (count(tag, word) + smoothing) / 
                        (count(tag) + smoothing * |vocab|)
        
        Args:
            tag (str): POS tag
            word (str): Kelime
            smoothing (float): Add-k smoothing parametresi
        
        Returns:
            float: Log olasÄ±lÄ±k deÄŸeri
        """
        word = word.lower()
        
        if word in self.vocab:
            # Bilinen kelime
            numerator = self.emission_counts[tag][word] + smoothing
            denominator = self.tag_counts[tag] + smoothing * len(self.vocab)
        else:
            # Bilinmeyen kelime (OOV)
            unk_prob = self._get_unk_prob(tag, word)
            numerator = unk_prob + smoothing
            denominator = self.tag_counts[tag] + smoothing * len(self.vocab)
        
        # SÄ±fÄ±r bÃ¶lme kontrolÃ¼
        if denominator == 0:
            return float('-inf')
        
        return math.log(numerator / denominator)
    
    def _get_unk_prob(self, tag, word):
        """
        Bilinmeyen kelimeler iÃ§in olasÄ±lÄ±k hesaplar.
        TÃ¼rkÃ§e morfolojik analiz kullanÄ±r.
        
        Args:
            tag (str): POS tag
            word (str): Bilinmeyen kelime
        
        Returns:
            float: OOV olasÄ±lÄ±k skoru (0-1 arasÄ±)
        """
        # TÃ¼rkÃ§e suffix analizi tabanlÄ± OOV handling
        suffix_scores = {
            'VERB': self._check_verb_suffixes(word),
            'NOUN': self._check_noun_suffixes(word),
            'ADJ': self._check_adj_suffixes(word),
            'ADV': self._check_adv_suffixes(word),
            'PROPN': self._check_propn_features(word),
            'NUM': self._check_num_features(word),
        }
        
        # Ä°lgili tag iÃ§in score, yoksa default
        return suffix_scores.get(tag, 0.05)
    
    def _check_verb_suffixes(self, word):
        """Fiil suffixlerini kontrol eder."""
        verb_suffixes = ['mak', 'mek', 'yor', 'dÄ±', 'di', 'du', 'dÃ¼', 
                        'tÄ±', 'ti', 'tu', 'tÃ¼', 'r', 'Ä±r', 'ir', 'ur', 'Ã¼r',
                        'miÅŸ', 'muÅŸ', 'mÃ¼ÅŸ', 'mÄ±ÅŸ']
        
        for suffix in verb_suffixes:
            if word.endswith(suffix):
                return 0.7
        return 0.1
    
    def _check_noun_suffixes(self, word):
        """Ä°sim suffixlerini kontrol eder."""
        noun_suffixes = ['lar', 'ler', 'lÄ±k', 'lik', 'luk', 'lÃ¼k',
                        'da', 'de', 'ta', 'te', 'dan', 'den', 'tan', 'ten',
                        'nÄ±n', 'nin', 'nun', 'nÃ¼n', 'Ä±n', 'in', 'un', 'Ã¼n']
        
        for suffix in noun_suffixes:
            if word.endswith(suffix):
                return 0.6
        return 0.2
    
    def _check_adj_suffixes(self, word):
        """SÄ±fat suffixlerini kontrol eder."""
        adj_suffixes = ['li', 'lÄ±', 'lu', 'lÃ¼', 'sÄ±z', 'siz', 'suz', 'sÃ¼z',
                       'sal', 'sel', 'ki']
        
        for suffix in adj_suffixes:
            if word.endswith(suffix):
                return 0.5
        return 0.1
    
    def _check_adv_suffixes(self, word):
        """Zarf suffixlerini kontrol eder."""
        adv_suffixes = ['ca', 'ce', 'Ã§a', 'Ã§e', 'arak', 'erek']
        
        for suffix in adv_suffixes:
            if word.endswith(suffix):
                return 0.6
        return 0.1
    
    def _check_propn_features(self, word):
        """Ã–zel isim Ã¶zelliklerini kontrol eder."""
        # Ä°lk harf bÃ¼yÃ¼k mÃ¼?
        if word and word[0].isupper():
            return 0.8
        return 0.1
    
    def _check_num_features(self, word):
        """SayÄ± Ã¶zelliklerini kontrol eder."""
        # Rakam iÃ§eriyor mu?
        if any(c.isdigit() for c in word):
            return 0.9
        
        # SayÄ± kelimeleri
        number_words = ['bir', 'iki', 'Ã¼Ã§', 'dÃ¶rt', 'beÅŸ', 'altÄ±', 'yedi', 
                       'sekiz', 'dokuz', 'on', 'yÃ¼z', 'bin', 'milyon']
        if word.lower() in number_words:
            return 0.8
            
        return 0.1
    
    def _print_count_statistics(self):
        """SayÄ±m istatistiklerini yazdÄ±rÄ±r."""
        print(f"\nğŸ“ˆ HMM SayÄ±m Ä°statistikleri:")
        print(f"{'='*40}")
        print(f"Toplam cÃ¼mle: {self.total_sentences:,}")
        print(f"Toplam token: {self.total_tokens:,}")
        print(f"Unique kelime: {len(self.vocab):,}")
        print(f"POS tag sayÄ±sÄ±: {len(self.tag_counts):,}")
        print(f"Transition pairs: {sum(len(v) for v in self.transition_counts.values()):,}")
        print(f"Emission pairs: {sum(len(v) for v in self.emission_counts.values()):,}")
        
        # En sÄ±k tag'leri gÃ¶ster
        most_common_tags = self.tag_counts.most_common(5)
        print(f"\nEn sÄ±k POS tag'leri:")
        for tag, count in most_common_tags:
            percentage = (count / self.total_tokens) * 100
            print(f"  {tag}: {count:,} (%{percentage:.1f})")
    
    def get_tag_vocabulary(self, tag):
        """
        Belirli bir tag iÃ§in kelime daÄŸarcÄ±ÄŸÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
        
        Args:
            tag (str): POS tag
        
        Returns:
            set: Bu tag ile gÃ¶rÃ¼len unique kelimeler
        """
        return set(self.emission_counts[tag].keys())
    
    def get_most_likely_tags(self, word, top_k=3):
        """
        Bir kelime iÃ§in en olasÄ± tag'leri dÃ¶ndÃ¼rÃ¼r.
        
        Args:
            word (str): Kelime
            top_k (int): KaÃ§ tane tag dÃ¶ndÃ¼rÃ¼lecek
        
        Returns:
            list: (tag, olasÄ±lÄ±k) tuple'larÄ±
        """
        word = word.lower()
        tag_probs = []
        
        for tag in self.tag_counts:
            prob = self.get_emission_prob(tag, word)
            tag_probs.append((tag, prob))
        
        # En yÃ¼ksek olasÄ±lÄ±klÄ± tag'leri dÃ¶ndÃ¼r
        tag_probs.sort(key=lambda x: x[1], reverse=True)
        return tag_probs[:top_k] 