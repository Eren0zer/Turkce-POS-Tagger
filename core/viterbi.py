"""
Viterbi Algorithm Implementation

Bu modÃ¼l Viterbi algoritmasÄ± ile HMM'den en olasÄ± state sequence'Ä±nÄ± (POS tag dizisi) 
bulur. Dynamic programming tabanlÄ± optimal decoding yapar.
"""

import math
from collections import defaultdict


class ViterbiDecoder:
    """
    Viterbi algoritmasÄ± ile POS tag sequence decoding yapan sÄ±nÄ±f.
    
    Viterbi algoritmasÄ±, HMM'de verilen observation sequence iÃ§in
    en olasÄ± hidden state sequence'Ä±nÄ± (POS tag dizisi) bulur.
    """
    
    def __init__(self, hmm_model):
        """
        ViterbiDecoder constructor.
        
        Args:
            hmm_model: EÄŸitilmiÅŸ HMMModel instance
        """
        self.model = hmm_model
        
        if not self.model.is_trained:
            raise ValueError("HMM model henÃ¼z eÄŸitilmemiÅŸ!")
    
    def decode(self, words):
        """
        Viterbi algoritmasÄ± ile kelime dizisi iÃ§in en olasÄ± POS tag dizisini bulur.
        
        Algorithm:
        1. Initialize: Ä°lk kelime iÃ§in tÃ¼m tag'lerin initial probabilities
        2. Forward pass: Her kelime iÃ§in tÃ¼m tag'leri optimize et  
        3. Backtrack: En iyi path'i geriye doÄŸru izle
        
        Args:
            words (list): Etiketlenecek kelimeler listesi
        
        Returns:
            list: En olasÄ± POS tag dizisi
        
        Example:
            >>> decoder = ViterbiDecoder(trained_model)
            >>> tags = decoder.decode(['Bu', 'kitap', 'gÃ¼zel'])
            >>> print(tags)  # ['PRON', 'NOUN', 'ADJ']
        """
        if not words:
            return []
        
        if len(words) == 1:
            # Tek kelime iÃ§in basit tahmin
            return [self.model.predict_tag(words[0])]
        
        n = len(words)
        tags = list(self.model.tags)
        
        if not tags:
            raise ValueError("Model'de hiÃ§ POS tag yok!")
        
        # Dynamic Programming tablolarÄ±
        # viterbi[t][tag] = t zamanÄ±ndaki tag iÃ§in en iyi log probability
        viterbi = defaultdict(lambda: defaultdict(lambda: float('-inf')))
        
        # backpointer[t][tag] = t zamanÄ±ndaki tag iÃ§in en iyi Ã¶nceki tag
        backpointer = defaultdict(dict)
        
        # Step 1: Initialize (t=0)
        self._initialize_viterbi(viterbi, backpointer, tags, words[0])
        
        # Step 2: Forward pass (t=1...n-1)
        for t in range(1, n):
            self._forward_step(viterbi, backpointer, tags, words[t], t)
        
        # Step 3: Find best final tag
        best_final_tag = self._find_best_final_tag(viterbi, tags, n-1)
        
        # Step 4: Backtrack to get best path
        best_path = self._backtrack_path(backpointer, best_final_tag, n-1)
        
        return best_path
    
    def _initialize_viterbi(self, viterbi, backpointer, tags, first_word):
        """
        Viterbi tablosunu ilk kelime iÃ§in initialize eder.
        
        Args:
            viterbi: Viterbi DP tablosu
            backpointer: Backpointer tablosu
            tags: POS tag listesi
            first_word: Ä°lk kelime
        """
        for tag in tags:
            # P(tag | <START>) * P(word | tag)
            trans_prob = self.model.get_transition_prob("<START>", tag)
            emis_prob = self.model.get_emission_prob(tag, first_word)
            
            viterbi[0][tag] = trans_prob + emis_prob
            backpointer[0][tag] = "<START>"
    
    def _forward_step(self, viterbi, backpointer, tags, word, t):
        """
        Viterbi forward step - t zamanÄ±ndaki tÃ¼m tag'ler iÃ§in optimize eder.
        
        Args:
            viterbi: Viterbi DP tablosu
            backpointer: Backpointer tablosu  
            tags: POS tag listesi
            word: t zamanÄ±ndaki kelime
            t: Zaman adÄ±mÄ±
        """
        for curr_tag in tags:
            max_prob = float('-inf')
            best_prev_tag = None
            
            # TÃ¼m Ã¶nceki tag'lerden geÃ§iÅŸleri dene
            for prev_tag in tags:
                # P(curr_tag | prev_tag) * P(word | curr_tag) * viterbi[t-1][prev_tag]
                trans_prob = self.model.get_transition_prob(prev_tag, curr_tag)
                emis_prob = self.model.get_emission_prob(curr_tag, word)
                
                # Log space'te toplama
                total_prob = viterbi[t-1][prev_tag] + trans_prob + emis_prob
                
                if total_prob > max_prob:
                    max_prob = total_prob
                    best_prev_tag = prev_tag
            
            viterbi[t][curr_tag] = max_prob
            backpointer[t][curr_tag] = best_prev_tag
    
    def _find_best_final_tag(self, viterbi, tags, final_time):
        """
        Final time step'te en iyi tag'i bulur.
        
        Args:
            viterbi: Viterbi DP tablosu
            tags: POS tag listesi
            final_time: Son zaman adÄ±mÄ±
        
        Returns:
            str: En iyi final tag
        """
        best_final_tag = None
        best_final_prob = float('-inf')
        
        for tag in tags:
            # CÃ¼mle sonu geÃ§iÅŸ olasÄ±lÄ±ÄŸÄ±nÄ± da dahil et
            final_prob = viterbi[final_time][tag]
            end_prob = self.model.get_transition_prob(tag, "<END>")
            total_prob = final_prob + end_prob
            
            if total_prob > best_final_prob:
                best_final_prob = total_prob
                best_final_tag = tag
        
        return best_final_tag if best_final_tag else tags[0]
    
    def _backtrack_path(self, backpointer, best_final_tag, final_time):
        """
        En iyi path'i geriye doÄŸru izleyerek tag sequence'Ä±nÄ± bulur.
        
        Args:
            backpointer: Backpointer tablosu
            best_final_tag: En iyi final tag
            final_time: Son zaman adÄ±mÄ±
        
        Returns:
            list: En iyi tag dizisi
        """
        path = [best_final_tag]
        
        # Geriye doÄŸru en iyi path'i izle
        for t in range(final_time, 0, -1):
            prev_tag = backpointer[t][path[-1]]
            path.append(prev_tag)
        
        # Path'i ters Ã§evir
        path.reverse()
        # EÄŸer path'in baÅŸÄ±nda <START> varsa onu Ã§Ä±kar
        if path and path[0] == "<START>":
            path = path[1:]
        return path
    
    def decode_with_probabilities(self, words):
        """
        Kelime dizisi iÃ§in tag sequence'Ä± ve olasÄ±lÄ±klarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
        
        Args:
            words (list): Etiketlenecek kelimeler
        
        Returns:
            tuple: (tag_sequence, log_probability)
        """
        if not words:
            return [], 0.0
        
        tags = self.decode(words)
        
        # Total log probability hesapla
        log_prob = 0.0
        prev_tag = "<START>"
        
        for word, tag in zip(words, tags):
            trans_prob = self.model.get_transition_prob(prev_tag, tag)
            emis_prob = self.model.get_emission_prob(tag, word)
            log_prob += trans_prob + emis_prob
            prev_tag = tag
        
        # CÃ¼mle sonu olasÄ±lÄ±ÄŸÄ±nÄ± ekle
        end_prob = self.model.get_transition_prob(prev_tag, "<END>")
        log_prob += end_prob
        
        return tags, log_prob
    
    def get_top_k_sequences(self, words, k=3):
        """
        En olasÄ± k tag sequence'Ä±nÄ± dÃ¶ndÃ¼rÃ¼r.
        
        Not: Bu implementasyon basit bir yaklaÅŸÄ±m kullanÄ±r.
        GerÃ§ek k-best Viterbi daha karmaÅŸÄ±ktÄ±r.
        
        Args:
            words (list): Etiketlenecek kelimeler
            k (int): KaÃ§ sequence dÃ¶ndÃ¼rÃ¼lecek
        
        Returns:
            list: (tag_sequence, log_probability) tuple'larÄ±
        """
        if not words or k <= 0:
            return []
        
        # Ä°lk olarak en iyi sequence'Ä± bul
        best_tags, best_prob = self.decode_with_probabilities(words)
        results = [(best_tags, best_prob)]
        
        # Basit yaklaÅŸÄ±m: Her pozisyonda 2. en iyi tag'i dene
        if k > 1 and len(words) > 1:
            for i in range(len(words)):
                # i. pozisyondaki tag'i deÄŸiÅŸtir
                alternative_tags = best_tags.copy()
                
                # Bu kelime iÃ§in 2. en iyi tag'i bul
                word = words[i]
                tag_probs = []
                for tag in self.model.tags:
                    prob = self.model.get_emission_prob(tag, word)
                    tag_probs.append((tag, prob))
                
                tag_probs.sort(key=lambda x: x[1], reverse=True)
                
                if len(tag_probs) > 1:
                    second_best_tag = tag_probs[1][0]
                    alternative_tags[i] = second_best_tag
                    
                    # Bu sequence'Ä±n olasÄ±lÄ±ÄŸÄ±nÄ± hesapla
                    alt_prob = self._calculate_sequence_probability(words, alternative_tags)
                    results.append((alternative_tags, alt_prob))
        
        # OlasÄ±lÄ±ÄŸa gÃ¶re sÄ±rala ve k tane dÃ¶ndÃ¼r
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:k]
    
    def _calculate_sequence_probability(self, words, tags):
        """
        Verilen word-tag sequence'Ä±nÄ±n log probability'sini hesaplar.
        
        Args:
            words (list): Kelimeler
            tags (list): Tag'ler
        
        Returns:
            float: Log probability
        """
        if len(words) != len(tags):
            return float('-inf')
        
        log_prob = 0.0
        prev_tag = "<START>"
        
        for word, tag in zip(words, tags):
            trans_prob = self.model.get_transition_prob(prev_tag, tag)
            emis_prob = self.model.get_emission_prob(tag, word)
            log_prob += trans_prob + emis_prob
            prev_tag = tag
        
        # CÃ¼mle sonu
        end_prob = self.model.get_transition_prob(prev_tag, "<END>")
        log_prob += end_prob
        
        return log_prob
    
    def print_decoding_details(self, words, show_probabilities=True):
        """
        Decoding detaylarÄ±nÄ± yazdÄ±rÄ±r (debugging iÃ§in).
        
        Args:
            words (list): Kelimeler
            show_probabilities (bool): OlasÄ±lÄ±klarÄ± gÃ¶ster
        """
        if not words:
            print("BoÅŸ kelime listesi")
            return
        
        tags, total_prob = self.decode_with_probabilities(words)
        
        print(f"\nğŸ” Viterbi Decoding DetaylarÄ±:")
        print(f"{'='*50}")
        print(f"Kelime sayÄ±sÄ±: {len(words)}")
        print(f"Toplam log probability: {total_prob:.4f}")
        print(f"Normalized probability: {math.exp(total_prob):.2e}")
        
        print(f"\nKelime â†’ Tag eÅŸleÅŸtirmesi:")
        for i, (word, tag) in enumerate(zip(words, tags)):
            if show_probabilities:
                emis_prob = self.model.get_emission_prob(tag, word)
                print(f"{i+1:2d}. {word:15s} â†’ {tag:8s} (log_prob: {emis_prob:.3f})")
            else:
                print(f"{i+1:2d}. {word:15s} â†’ {tag}")
        
        if show_probabilities:
            print(f"\nTransition probabilities:")
            prev_tag = "<START>"
            for i, tag in enumerate(tags):
                trans_prob = self.model.get_transition_prob(prev_tag, tag)
                print(f"{prev_tag:8s} â†’ {tag:8s}: {trans_prob:.3f}")
                prev_tag = tag
            
            end_prob = self.model.get_transition_prob(prev_tag, "<END>")
            print(f"{prev_tag:8s} â†’ <END>    : {end_prob:.3f}") 