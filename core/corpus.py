"""
CoNLL-U Format Corpus Reader

Bu modÃ¼l CoNLL-U formatÄ±ndaki dosyalarÄ± okumak ve parse etmek iÃ§in kullanÄ±lÄ±r.
CoNLL-U formatÄ± Universal Dependencies projesi tarafÄ±ndan kullanÄ±lan standart formattÄ±r.
"""

class CoNLLUReader:
    """
    CoNLL-U formatÄ±ndaki dosyalarÄ± okuyup parse eden sÄ±nÄ±f.
    
    CoNLL-U formatÄ± ÅŸu sÃ¼tunlarÄ± iÃ§erir:
    ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC
    """
    
    def __init__(self, filepath):
        """
        CoNLLUReader constructor.
        
        Args:
            filepath (str): Okunacak CoNLL-U dosyasÄ±nÄ±n yolu
        """
        self.filepath = filepath
        self._sentences = None  # Cache for sentences
    
    def __iter__(self):
        """Iterator interface for sentence iteration."""
        if self._sentences is None:
            self._sentences = self.read_sentences()
        return iter(self._sentences)
    
    def read_sentences(self):
        """
        CoNLL-U dosyasÄ±ndan tÃ¼m cÃ¼mleleri okur ve parse eder.
        
        Returns:
            list: Her cÃ¼mle iÃ§in token listesi iÃ§eren liste.
                  Her token bir dict'tir: {'form': str, 'upos': str}
        
        Example:
            >>> reader = CoNLLUReader('data/train.conllu')
            >>> sentences = reader.read_sentences()
            >>> print(sentences[0])
            [{'form': 'Bu', 'upos': 'PRON'}, {'form': 'kitap', 'upos': 'NOUN'}]
        """
        sentences = []
        current_sentence = []
        
        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    
                    # BoÅŸ satÄ±r veya yorum satÄ±rÄ± - cÃ¼mle bitiÅŸi
                    if line.startswith('#') or line == '':
                        if current_sentence:
                            sentences.append(current_sentence)
                            current_sentence = []
                        continue
                    
                    # Token satÄ±rÄ±nÄ± parse et
                    token = self._parse_token_line(line, line_num)
                    if token:
                        current_sentence.append(token)
                
                # Dosya sonunda kalan cÃ¼mle varsa ekle
                if current_sentence:
                    sentences.append(current_sentence)
        
        except FileNotFoundError:
            raise FileNotFoundError(f"CoNLL-U dosyasÄ± bulunamadÄ±: {self.filepath}")
        except UnicodeDecodeError:
            raise UnicodeDecodeError(f"Dosya encoding hatasÄ±: {self.filepath}. UTF-8 encoding gerekli.")
        
        print(f"âœ… {len(sentences)} cÃ¼mle baÅŸarÄ±yla okundu: {self.filepath}")
        self._sentences = sentences  # Cache sentences
        return sentences
    
    def _parse_token_line(self, line, line_num):
        """
        Tek bir token satÄ±rÄ±nÄ± parse eder.
        
        Args:
            line (str): Parse edilecek satÄ±r
            line_num (int): SatÄ±r numarasÄ± (hata raporlama iÃ§in)
        
        Returns:
            dict or None: Token bilgilerini iÃ§eren dict veya hatalÄ± satÄ±r iÃ§in None
        """
        parts = line.split('\t')
        
        # CoNLL-U formatÄ±nda en az 10 sÃ¼tun olmalÄ±
        if len(parts) < 10:
            print(f"âš ï¸  HatalÄ± satÄ±r format (satÄ±r {line_num}): {line[:50]}...")
            return None
        
        try:
            # Sadece basit token ID'leri al (1, 2, 3...), range'leri atla (1-2, 3-4...)
            token_id = parts[0]
            if '-' in token_id or '.' in token_id:
                return None
            
            form = parts[1]      # Kelimenin surface formu
            lemma = parts[2]     # Kelimenin lemmasÄ±
            upos = parts[3]      # Universal POS tag
            
            # BoÅŸ alanlarÄ± kontrol et
            if not form or not upos or form == '_' or upos == '_':
                print(f"âš ï¸  Eksik bilgi (satÄ±r {line_num}): form='{form}', upos='{upos}'")
                return None
            
            return {
                'form': form,
                'lemma': lemma if lemma != '_' else form,
                'upos': upos
            }
            
        except (IndexError, ValueError) as e:
            print(f"âš ï¸  Token parse hatasÄ± (satÄ±r {line_num}): {e}")
            return None
    
    def get_vocabulary(self, sentences=None):
        """
        Corpus'taki tÃ¼m unique kelimeleri dÃ¶ndÃ¼rÃ¼r.
        
        Args:
            sentences (list, optional): CÃ¼mle listesi. None ise dosyadan okur.
        
        Returns:
            set: Unique kelimeler seti (kÃ¼Ã§Ã¼k harfe Ã§evrilmiÅŸ)
        """
        if sentences is None:
            sentences = self.read_sentences()
        
        vocabulary = set()
        for sentence in sentences:
            for token in sentence:
                vocabulary.add(token['form'].lower())
        
        return vocabulary
    
    def get_tag_set(self, sentences=None):
        """
        Corpus'taki tÃ¼m unique POS etiketlerini dÃ¶ndÃ¼rÃ¼r.
        
        Args:
            sentences (list, optional): CÃ¼mle listesi. None ise dosyadan okur.
        
        Returns:
            set: Unique POS etiketleri seti
        """
        if sentences is None:
            sentences = self.read_sentences()
        
        tags = set()
        for sentence in sentences:
            for token in sentence:
                tags.add(token['upos'])
        
        return tags
    
    def print_statistics(self, sentences=None):
        """
        Corpus istatistiklerini yazdÄ±rÄ±r.
        
        Args:
            sentences (list, optional): CÃ¼mle listesi. None ise dosyadan okur.
        """
        if sentences is None:
            sentences = self.read_sentences()
        
        total_tokens = sum(len(sent) for sent in sentences)
        vocabulary = self.get_vocabulary(sentences)
        tags = self.get_tag_set(sentences)
        
        avg_sent_length = total_tokens / len(sentences) if sentences else 0
        
        print(f"\nğŸ“Š Corpus Ä°statistikleri - {self.filepath}")
        print(f"{'='*50}")
        print(f"Toplam cÃ¼mle sayÄ±sÄ±: {len(sentences):,}")
        print(f"Toplam token sayÄ±sÄ±: {total_tokens:,}")
        print(f"Ortalama cÃ¼mle uzunluÄŸu: {avg_sent_length:.1f}")
        print(f"Unique kelime sayÄ±sÄ±: {len(vocabulary):,}")
        print(f"POS etiket sayÄ±sÄ±: {len(tags)}")
        print(f"POS etiketleri: {', '.join(sorted(tags))}")
    
    def get_sentence_count(self):
        """
        Corpus'taki toplam cÃ¼mle sayÄ±sÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
        
        Returns:
            int: CÃ¼mle sayÄ±sÄ±
        """
        if self._sentences is None:
            self._sentences = self.read_sentences()
        return len(self._sentences)
    
    def get_token_count(self):
        """
        Corpus'taki toplam token sayÄ±sÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
        
        Returns:
            int: Token sayÄ±sÄ±
        """
        if self._sentences is None:
            self._sentences = self.read_sentences()
        return sum(len(sent) for sent in self._sentences)
    
    def get_tag_statistics(self):
        """
        POS tag'larÄ±n frekans istatistiklerini dÃ¶ndÃ¼rÃ¼r.
        
        Returns:
            dict: {tag: count} formatÄ±nda istatistikler
        """
        if self._sentences is None:
            self._sentences = self.read_sentences()
        
        tag_counts = {}
        for sentence in self._sentences:
            for token in sentence:
                tag = token['upos']
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        return tag_counts
    
    def get_word_statistics(self):
        """
        Kelimelerin frekans istatistiklerini dÃ¶ndÃ¼rÃ¼r.
        
        Returns:
            dict: {word: count} formatÄ±nda istatistikler
        """
        if self._sentences is None:
            self._sentences = self.read_sentences()
        
        word_counts = {}
        for sentence in self._sentences:
            for token in sentence:
                word = token['form'].lower()
                word_counts[word] = word_counts.get(word, 0) + 1
        
        return word_counts 