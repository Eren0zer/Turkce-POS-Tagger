#!/usr/bin/env python3
"""
Data Preprocessing Script - Excel to CoNLL-U Conversion

Bu script Excel dosyasÄ±ndaki TÃ¼rkÃ§e cÃ¼mleleri Stanza ile iÅŸleyerek
CoNLL-U formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve train/dev/test olarak bÃ¶ler.

Usage:
    python scripts/01_preprocess_data.py
    
Input:
    - data/raw/dataset.xlsx (Excel dosyasÄ±)
    
Output:
    - data/processed/train.conllu
    - data/processed/dev.conllu  
    - data/processed/test.conllu
"""

import os
import sys
import pandas as pd
import stanza
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# KonfigÃ¼rasyon
INPUT_EXCEL = 'data/raw/dataset.xlsx'
OUTPUT_DIR = 'data/processed/'
SENTENCE_COLUMN = 'CÃ¼mle'  # Excel'deki cÃ¼mle sÃ¼tunu adÄ±
BATCH_SIZE = 50               # Stanza iÅŸleme batch boyutu
TEST_SIZE = 0.2              # %20 test
DEV_SIZE = 0.5               # Test'in %50'si dev (toplam %10)
RANDOM_STATE = 42            # Reproducible split


def setup_stanza():
    """
    Stanza TÃ¼rkÃ§e modelini yÃ¼kler. Gerekirse indirir.
    
    Returns:
        stanza.Pipeline: TÃ¼rkÃ§e NLP pipeline
    """
    print("ğŸ”§ Stanza TÃ¼rkÃ§e modeli yÃ¼kleniyor...")
    
    try:
        # Ã–nce model var mÄ± kontrol et
        nlp = stanza.Pipeline('tr', processors='tokenize,pos', verbose=False)
        print("âœ… Stanza modeli baÅŸarÄ±yla yÃ¼klendi")
        return nlp
    except Exception as e:
        print(f"âš ï¸  Stanza modeli bulunamadÄ±, indiriliyor: {e}")
        try:
            stanza.download('tr')
            nlp = stanza.Pipeline('tr', processors='tokenize,pos', verbose=False)
            print("âœ… Stanza modeli indirildi ve yÃ¼klendi")
            return nlp
        except Exception as e:
            raise RuntimeError(f"Stanza modeli yÃ¼klenemedi: {e}")


def load_excel_data(filepath):
    """
    Excel dosyasÄ±ndan cÃ¼mleleri yÃ¼kler ve temizler.
    
    Args:
        filepath (str): Excel dosyasÄ± yolu
    
    Returns:
        list: TemizlenmiÅŸ cÃ¼mle listesi
    """
    print(f"ğŸ“Š Excel dosyasÄ± okunuyor: {filepath}")
    
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Excel dosyasÄ± bulunamadÄ±: {filepath}")
    
    try:
        # Excel dosyasÄ±nÄ± oku
        df = pd.read_excel(filepath)
        print(f"Excel dosyasÄ± okundu - {len(df)} satÄ±r")
        
        # CÃ¼mle sÃ¼tununu kontrol et
        if SENTENCE_COLUMN not in df.columns:
            available_columns = ', '.join(df.columns.tolist())
            raise ValueError(f"'{SENTENCE_COLUMN}' sÃ¼tunu bulunamadÄ±. Mevcut sÃ¼tunlar: {available_columns}")
        
        # CÃ¼mleleri Ã§Ä±kar ve temizle
        sentences = df[SENTENCE_COLUMN].dropna().tolist()
        
        # Temel temizlik
        cleaned_sentences = []
        for sent in sentences:
            if isinstance(sent, str):
                sent = sent.strip()
                # Ã‡ok kÄ±sa veya Ã§ok uzun cÃ¼mleleri filtrele
                if 5 <= len(sent) <= 500 and len(sent.split()) >= 2:
                    cleaned_sentences.append(sent)
        
        print(f"âœ… {len(cleaned_sentences)} geÃ§erli cÃ¼mle bulundu")
        return cleaned_sentences
        
    except Exception as e:
        raise RuntimeError(f"Excel dosyasÄ± okunamadÄ±: {e}")


def process_sentences_with_stanza(sentences, nlp, batch_size=BATCH_SIZE):
    """
    CÃ¼mleleri Stanza ile iÅŸleyerek token bilgilerini Ã§Ä±karÄ±r.
    
    Args:
        sentences (list): Ä°ÅŸlenecek cÃ¼mle listesi
        nlp: Stanza pipeline
        batch_size (int): Batch boyutu
    
    Returns:
        list: Ä°ÅŸlenmiÅŸ cÃ¼mle listesi (her cÃ¼mle token dict'leri iÃ§erir)
    """
    print(f"ğŸ”¤ Stanza ile {len(sentences)} cÃ¼mle iÅŸleniyor...")
    
    processed_data = []
    failed_count = 0
    
    # Progress bar ile batch'ler halinde iÅŸle
    for i in tqdm(range(0, len(sentences), batch_size), desc="Processing batches"):
        batch = sentences[i:i+batch_size]
        
        for sent_text in batch:
            try:
                # Stanza ile iÅŸle
                doc = nlp(sent_text)
                tokens = []
                
                # Her sentence'daki word'leri al
                for sentence in doc.sentences:
                    for word in sentence.words:
                        tokens.append({
                            'text': word.text,
                            'lemma': word.lemma if word.lemma else word.text,
                            'upos': word.upos if word.upos else 'X'
                        })
                
                # En az 2 token olmasÄ± gerekiyor
                if len(tokens) >= 2:
                    processed_data.append(tokens)
                else:
                    failed_count += 1
                    
            except Exception as e:
                failed_count += 1
                if failed_count <= 5:  # Ä°lk 5 hatayÄ± gÃ¶ster
                    print(f"âš ï¸  Ä°ÅŸleme hatasÄ±: {sent_text[:50]}... -> {e}")
    
    print(f"âœ… {len(processed_data)} cÃ¼mle baÅŸarÄ±yla iÅŸlendi")
    if failed_count > 0:
        print(f"âš ï¸  {failed_count} cÃ¼mle iÅŸlenemedi")
    
    return processed_data


def to_conllu_format(processed_sentences):
    """
    Ä°ÅŸlenmiÅŸ cÃ¼mleleri CoNLL-U formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    
    Args:
        processed_sentences (list): Ä°ÅŸlenmiÅŸ cÃ¼mle listesi
    
    Returns:
        str: CoNLL-U format string
    """
    conllu_lines = []
    
    for sent_id, sentence in enumerate(processed_sentences, 1):
        # Sentence metadata
        sentence_text = ' '.join([token['text'] for token in sentence])
        conllu_lines.append(f"# sent_id = {sent_id}")
        conllu_lines.append(f"# text = {sentence_text}")
        
        # Tokens
        for token_id, token in enumerate(sentence, 1):
            # CoNLL-U format: ID FORM LEMMA UPOS XPOS FEATS HEAD DEPREL DEPS MISC
            line = (f"{token_id}\t{token['text']}\t{token['lemma']}\t{token['upos']}\t"
                   f"_\t_\t0\troot\t_\t_")
            conllu_lines.append(line)
        
        # BoÅŸ satÄ±r (sentence separator)
        conllu_lines.append("")
    
    return "\n".join(conllu_lines)


def split_and_save_data(processed_sentences):
    """
    Veriyi train/dev/test olarak bÃ¶ler ve CoNLL-U dosyalarÄ±nÄ± kaydeder.
    
    Args:
        processed_sentences (list): Ä°ÅŸlenmiÅŸ cÃ¼mle listesi
    """
    print(f"ğŸ“‚ Veri bÃ¶lÃ¼nÃ¼yor ve kaydediliyor...")
    
    # Ã‡Ä±ktÄ± dizinini oluÅŸtur
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Veriyi bÃ¶l
    train_data, temp_data = train_test_split(
        processed_sentences, 
        test_size=TEST_SIZE, 
        random_state=RANDOM_STATE
    )
    
    dev_data, test_data = train_test_split(
        temp_data, 
        test_size=DEV_SIZE, 
        random_state=RANDOM_STATE
    )
    
    # CoNLL-U dosyalarÄ±nÄ± kaydet
    datasets = [
        (train_data, f'{OUTPUT_DIR}/train.conllu', 'Training'),
        (dev_data, f'{OUTPUT_DIR}/dev.conllu', 'Development'),
        (test_data, f'{OUTPUT_DIR}/test.conllu', 'Test')
    ]
    
    total_tokens = 0
    for data, filename, description in datasets:
        conllu_content = to_conllu_format(data)
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(conllu_content)
        
        tokens_in_split = sum(len(sent) for sent in data)
        total_tokens += tokens_in_split
        
        print(f"âœ… {description:12s}: {len(data):4d} cÃ¼mle, {tokens_in_split:5d} token â†’ {filename}")
    
    print(f"\nğŸ“Š Veri BÃ¶lme Ã–zeti:")
    print(f"{'='*50}")
    print(f"Toplam cÃ¼mle: {len(processed_sentences):,}")
    print(f"Toplam token: {total_tokens:,}")
    print(f"Train: {len(train_data):,} cÃ¼mle (%{len(train_data)/len(processed_sentences)*100:.1f})")
    print(f"Dev  : {len(dev_data):,} cÃ¼mle (%{len(dev_data)/len(processed_sentences)*100:.1f})")
    print(f"Test : {len(test_data):,} cÃ¼mle (%{len(test_data)/len(processed_sentences)*100:.1f})")


def validate_output_files():
    """
    OluÅŸturulan CoNLL-U dosyalarÄ±nÄ± doÄŸrular.
    """
    print(f"\nğŸ” Ã‡Ä±ktÄ± dosyalarÄ± doÄŸrulanÄ±yor...")
    
    files_to_check = [
        f'{OUTPUT_DIR}/train.conllu',
        f'{OUTPUT_DIR}/dev.conllu', 
        f'{OUTPUT_DIR}/test.conllu'
    ]
    
    for filepath in files_to_check:
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            sentences = len([line for line in lines if line.startswith('# sent_id')])
            tokens = len([line for line in lines if line and not line.startswith('#') and line.strip()])
            
            file_size = os.path.getsize(filepath) / 1024  # KB
            print(f"âœ… {os.path.basename(filepath):12s}: {sentences:3d} cÃ¼mle, {tokens:4d} token, {file_size:.1f} KB")
        else:
            print(f"âŒ {filepath} dosyasÄ± bulunamadÄ±!")


def print_sample_output():
    """
    Ã–rnek Ã§Ä±ktÄ± gÃ¶sterir.
    """
    train_file = f'{OUTPUT_DIR}/train.conllu'
    if os.path.exists(train_file):
        print(f"\nğŸ“‹ Ã–rnek Ã§Ä±ktÄ± (train.conllu'dan ilk birkaÃ§ satÄ±r):")
        print(f"{'='*60}")
        
        with open(train_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # Ä°lk cÃ¼mleyi gÃ¶ster
        for i, line in enumerate(lines):
            if i >= 15:  # Ä°lk 15 satÄ±r
                break
            print(line.rstrip())


def main():
    """
    Ana iÅŸlem fonksiyonu.
    """
    print("ğŸš€ Veri Ã–n Ä°ÅŸleme BaÅŸlÄ±yor")
    print("=" * 50)
    
    try:
        # 1. Stanza'yÄ± hazÄ±rla
        nlp = setup_stanza()
        
        # 2. Excel verilerini yÃ¼kle
        sentences = load_excel_data(INPUT_EXCEL)
        
        # 3. Stanza ile iÅŸle
        processed_sentences = process_sentences_with_stanza(sentences, nlp)
        
        if not processed_sentences:
            raise ValueError("HiÃ§ cÃ¼mle iÅŸlenemedi!")
        
        # 4. Veriyi bÃ¶l ve kaydet
        split_and_save_data(processed_sentences)
        
        # 5. DoÄŸrulama
        validate_output_files()
        
        # 6. Ã–rnek Ã§Ä±ktÄ±
        print_sample_output()
        
        print(f"\nğŸ‰ Veri Ã¶n iÅŸleme baÅŸarÄ±yla tamamlandÄ±!")
        print(f"Sonraki adÄ±m: python scripts/02_train_model.py")
        
    except Exception as e:
        print(f"\nâŒ Hata: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main() 