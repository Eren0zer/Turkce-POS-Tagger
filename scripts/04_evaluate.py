#!/usr/bin/env python3
"""
HMM Model Evaluation Script

Bu script eÄŸitilmiÅŸ HMM modelini test seti Ã¼zerinde kapsamlÄ± ÅŸekilde 
deÄŸerlendirir ve detaylÄ± performans raporlarÄ± oluÅŸturur.

Usage:
    python scripts/04_evaluate.py
    
Input:
    - models/hmm_model.pkl (eÄŸitilmiÅŸ model)
    - data/processed/test.conllu (test seti)
    
Output:
    - models/evaluation_report.txt (detaylÄ± rapor)
    - models/confusion_matrix.png (confusion matrix)
    - models/performance_plots.png (performans grafikleri)
"""

import os
import sys
import time
import json
from collections import defaultdict, Counter
from datetime import datetime
from sklearn.metrics import precision_score, recall_score, f1_score

# Import iÃ§in gerekli path ayarÄ±
sys.path.append(os.path.abspath('.'))
from core import HMMModel, CoNLLUReader

# Plotting iÃ§in
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    import pandas as pd
    import numpy as np
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    print("âš ï¸  Matplotlib/Seaborn bulunamadÄ±. Grafikler oluÅŸturulamayacak.")

# KonfigÃ¼rasyon
MODEL_FILE = 'models/hmm_model.pkl'
TEST_FILE = 'data/processed/test.conllu'
OUTPUT_DIR = 'models/'
EVALUATION_REPORT = f'{OUTPUT_DIR}/evaluation_report.txt'
CONFUSION_MATRIX_PNG = f'{OUTPUT_DIR}/confusion_matrix.png'
PERFORMANCE_PLOTS_PNG = f'{OUTPUT_DIR}/performance_plots.png'
RESULTS_JSON = f'{OUTPUT_DIR}/evaluation_results.json'

# Evaluation ayarlarÄ±
MAX_SENTENCES = None          # None = tÃ¼m test setini kullan
DETAILED_ERRORS = True        # Hata Ã¶rneklerini gÃ¶ster
CREATE_PLOTS = True           # Grafik oluÅŸtur (eÄŸer kÃ¼tÃ¼phaneler varsa)


def load_model_and_test_data():
    """
    Model ve test verisini yÃ¼kler.
    
    Returns:
        tuple: (model, test_reader)
    """
    print(f"ğŸ“‚ Model ve test verisi yÃ¼kleniyor...")
    
    # Model kontrolÃ¼
    if not os.path.exists(MODEL_FILE):
        raise FileNotFoundError(f"Model dosyasÄ± bulunamadÄ±: {MODEL_FILE}")
    
    # Test dosyasÄ± kontrolÃ¼
    if not os.path.exists(TEST_FILE):
        raise FileNotFoundError(f"Test dosyasÄ± bulunamadÄ±: {TEST_FILE}")
    
    # Model yÃ¼kle
    print(f"   ğŸ“– Model: {MODEL_FILE}")
    model = HMMModel.load(MODEL_FILE)
    
    # Test verisi yÃ¼kle
    print(f"   ğŸ“– Test: {TEST_FILE}")
    test_reader = CoNLLUReader(TEST_FILE)
    
    print(f"âœ… Model ve test verisi yÃ¼klendi")
    print(f"   Test cÃ¼mle sayÄ±sÄ±: {test_reader.get_sentence_count():,}")
    print(f"   Test token sayÄ±sÄ±: {test_reader.get_token_count():,}")
    
    return model, test_reader


def evaluate_model(model, test_reader):
    """
    Modeli test seti Ã¼zerinde deÄŸerlendirir.
    
    Args:
        model: HMM model
        test_reader: Test verisi reader
    
    Returns:
        dict: DeÄŸerlendirme sonuÃ§larÄ±
    """
    print(f"\nğŸ” Model DeÄŸerlendirme BaÅŸlÄ±yor")
    print(f"{'='*50}")
    
    # SonuÃ§ toplama deÄŸiÅŸkenleri
    total_tokens = 0
    correct_predictions = 0
    sentences_processed = 0
    
    # POS tag bazlÄ± istatistikler
    tag_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
    
    # Confusion matrix iÃ§in
    true_tags = []
    predicted_tags = []
    
    # Hata analizi iÃ§in
    error_examples = []
    
    # Zaman Ã¶lÃ§Ã¼mÃ¼
    start_time = time.time()
    processing_times = []
    
    print(f"ğŸ“Š Test setindeki cÃ¼mleler iÅŸleniyor...")
    
    for sentence_idx, sentence_data in enumerate(test_reader):
        if MAX_SENTENCES and sentence_idx >= MAX_SENTENCES:
            break
        
        # CÃ¼mle verilerini hazÄ±rla
        words = [token['form'] for token in sentence_data]
        gold_tags = [token['upos'] for token in sentence_data]
        
        # Model tahminini al
        sentence_start = time.time()
        try:
            pred_tags = model.viterbi_decode(words)
            sentence_time = time.time() - sentence_start
            processing_times.append(sentence_time)
            
            # Ä°statistikleri gÃ¼ncelle
            sentences_processed += 1
            total_tokens += len(words)
            
            # Token bazlÄ± accuracy
            for gold_tag, pred_tag in zip(gold_tags, pred_tags):
                true_tags.append(gold_tag)
                predicted_tags.append(pred_tag)
                
                tag_stats[gold_tag]['total'] += 1
                
                if gold_tag == pred_tag:
                    correct_predictions += 1
                    tag_stats[gold_tag]['correct'] += 1
                else:
                    # Hata Ã¶rneÄŸi kaydet
                    if len(error_examples) < 50:  # Ä°lk 50 hatayÄ± kaydet
                        error_examples.append({
                            'sentence_id': sentence_idx + 1,
                            'word': words[gold_tags.index(gold_tag)] if gold_tag in gold_tags else 'UNK',
                            'true_tag': gold_tag,
                            'predicted_tag': pred_tag,
                            'context': ' '.join(words)
                        })
            
            # Progress gÃ¶ster
            if sentences_processed % 100 == 0:
                current_accuracy = (correct_predictions / total_tokens) * 100
                print(f"   Ä°ÅŸlenen: {sentences_processed:4d} cÃ¼mle, "
                      f"GeÃ§ici accuracy: %{current_accuracy:.2f}")
                
        except Exception as e:
            print(f"âŒ CÃ¼mle {sentence_idx + 1} iÅŸlenemedi: {e}")
            continue
    
    total_time = time.time() - start_time
    
    # SonuÃ§larÄ± hesapla
    overall_accuracy = (correct_predictions / total_tokens) * 100 if total_tokens > 0 else 0
    avg_sentence_time = sum(processing_times) / len(processing_times) if processing_times else 0
    tokens_per_second = total_tokens / total_time if total_time > 0 else 0
    
    # TÃ¼m tokenlar iÃ§in overall precision, recall, f1 (macro average)
    overall_precision = precision_score(true_tags, predicted_tags, average='macro', zero_division=0)
    overall_recall = recall_score(true_tags, predicted_tags, average='macro', zero_division=0)
    overall_f1 = f1_score(true_tags, predicted_tags, average='macro', zero_division=0)
    
    print(f"\nâœ… DeÄŸerlendirme tamamlandÄ±!")
    print(f"   Ä°ÅŸlenen cÃ¼mle: {sentences_processed:,}")
    print(f"   Ä°ÅŸlenen token: {total_tokens:,}")
    print(f"   DoÄŸru tahmin: {correct_predictions:,}")
    print(f"   Genel accuracy: %{overall_accuracy:.2f}")
    print(f"   Toplam sÃ¼re: {total_time:.2f} saniye")
    print(f"   Token/saniye: {tokens_per_second:.1f}")
    
    # SonuÃ§larÄ± dÃ¶nder
    results = {
        'sentences_processed': sentences_processed,
        'total_tokens': total_tokens,
        'correct_predictions': correct_predictions,
        'overall_accuracy': overall_accuracy / 100,  # 0-1 arasÄ± olacak ÅŸekilde
        'overall_precision': overall_precision,
        'overall_recall': overall_recall,
        'overall_f1': overall_f1,
        'total_time': total_time,
        'avg_sentence_time': avg_sentence_time,
        'tokens_per_second': tokens_per_second,
        'tag_stats': dict(tag_stats),
        'true_tags': true_tags,
        'predicted_tags': predicted_tags,
        'error_examples': error_examples,
        'processing_times': processing_times
    }
    
    return results


def analyze_per_tag_performance(results):
    """
    POS tag bazlÄ± performans analizi yapar.
    
    Args:
        results (dict): DeÄŸerlendirme sonuÃ§larÄ±
    
    Returns:
        dict: Tag bazlÄ± performans metrikleri
    """
    print(f"\nğŸ“ˆ POS Tag BazlÄ± Performans Analizi")
    print(f"{'='*60}")
    
    tag_stats = results['tag_stats']
    tag_performance = {}
    
    print(f"{'Tag':10s} {'Total':8s} {'Correct':8s} {'Accuracy':10s} {'Precision':10s} {'Recall':10s} {'F1':8s}")
    print(f"{'-'*70}")
    
    # Her tag iÃ§in precision, recall, f1 hesapla
    for tag in sorted(tag_stats.keys()):
        total = tag_stats[tag]['total']
        correct = tag_stats[tag]['correct']
        accuracy = (correct / total * 100) if total > 0 else 0
        
        # Precision ve recall iÃ§in true/false positives hesapla
        true_positives = correct
        false_positives = results['predicted_tags'].count(tag) - true_positives
        false_negatives = total - true_positives
        
        precision = (true_positives / (true_positives + false_positives)) * 100 if (true_positives + false_positives) > 0 else 0
        recall = (true_positives / (true_positives + false_negatives)) * 100 if (true_positives + false_negatives) > 0 else 0
        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0
        
        tag_performance[tag] = {
            'total': total,
            'correct': correct,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
        
        print(f"{tag:10s} {total:8d} {correct:8d} {accuracy:9.2f}% {precision:9.2f}% {recall:9.2f}% {f1:7.2f}")
    
    return tag_performance


def create_confusion_matrix(results):
    """
    Confusion matrix oluÅŸturur ve kaydeder.
    
    Args:
        results (dict): DeÄŸerlendirme sonuÃ§larÄ±
    """
    if not PLOTTING_AVAILABLE or not CREATE_PLOTS:
        print(f"âš ï¸  Grafik kÃ¼tÃ¼phaneleri bulunamadÄ± veya grafik oluÅŸturma kapalÄ±")
        return
    
    print(f"\nğŸ“Š Confusion Matrix oluÅŸturuluyor...")
    
    try:
        true_tags = results['true_tags']
        pred_tags = results['predicted_tags']
        
        # Unique taglarÄ± al
        all_tags = sorted(list(set(true_tags + pred_tags)))
        
        # Confusion matrix oluÅŸtur
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(true_tags, pred_tags, labels=all_tags)
        
        # Normalize et
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # Plot oluÅŸtur
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm_normalized, 
                   xticklabels=all_tags, 
                   yticklabels=all_tags,
                   annot=True, 
                   fmt='.2f', 
                   cmap='Blues',
                   cbar_kws={'label': 'Normalized Frequency'})
        
        plt.title('POS Tagging Confusion Matrix (Normalized)')
        plt.xlabel('Predicted Tags')
        plt.ylabel('True Tags')
        plt.xticks(rotation=45)
        plt.yticks(rotation=45)
        plt.tight_layout()
        
        # Kaydet
        plt.savefig(CONFUSION_MATRIX_PNG, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Confusion matrix kaydedildi: {CONFUSION_MATRIX_PNG}")
        
    except Exception as e:
        print(f"âŒ Confusion matrix oluÅŸturulamadÄ±: {e}")


def create_performance_plots(results, tag_performance):
    """
    Performans grafikleri oluÅŸturur.
    
    Args:
        results (dict): DeÄŸerlendirme sonuÃ§larÄ±
        tag_performance (dict): Tag bazlÄ± performans
    """
    if not PLOTTING_AVAILABLE or not CREATE_PLOTS:
        return
    
    print(f"\nğŸ“ˆ Performans grafikleri oluÅŸturuluyor...")
    
    try:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Tag frequency vs accuracy
        tags = list(tag_performance.keys())
        frequencies = [tag_performance[tag]['total'] for tag in tags]
        accuracies = [tag_performance[tag]['accuracy'] for tag in tags]
        
        ax1.scatter(frequencies, accuracies, alpha=0.7)
        ax1.set_xlabel('Tag Frequency')
        ax1.set_ylabel('Accuracy (%)')
        ax1.set_title('Tag Frequency vs Accuracy')
        ax1.grid(True, alpha=0.3)
        
        # 2. Precision vs Recall
        precisions = [tag_performance[tag]['precision'] for tag in tags]
        recalls = [tag_performance[tag]['recall'] for tag in tags]
        
        ax2.scatter(recalls, precisions, alpha=0.7)
        ax2.set_xlabel('Recall (%)')
        ax2.set_ylabel('Precision (%)')
        ax2.set_title('Precision vs Recall by Tag')
        ax2.grid(True, alpha=0.3)
        
        # Diagonal line ekle
        ax2.plot([0, 100], [0, 100], 'r--', alpha=0.5)
        
        # 3. Top 10 tags by frequency
        top_tags = sorted(tags, key=lambda x: tag_performance[x]['total'], reverse=True)[:10]
        top_freqs = [tag_performance[tag]['total'] for tag in top_tags]
        
        ax3.bar(range(len(top_tags)), top_freqs)
        ax3.set_xlabel('POS Tags')
        ax3.set_ylabel('Frequency')
        ax3.set_title('Top 10 Most Frequent Tags')
        ax3.set_xticks(range(len(top_tags)))
        ax3.set_xticklabels(top_tags, rotation=45)
        
        # 4. Processing time distribution
        times = results['processing_times']
        ax4.hist(times, bins=30, alpha=0.7, edgecolor='black')
        ax4.set_xlabel('Processing Time (seconds)')
        ax4.set_ylabel('Frequency')
        ax4.set_title('Sentence Processing Time Distribution')
        ax4.axvline(np.mean(times), color='red', linestyle='--', 
                   label=f'Mean: {np.mean(times):.3f}s')
        ax4.legend()
        
        plt.tight_layout()
        plt.savefig(PERFORMANCE_PLOTS_PNG, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Performans grafikleri kaydedildi: {PERFORMANCE_PLOTS_PNG}")
        
    except Exception as e:
        print(f"âŒ Performans grafikleri oluÅŸturulamadÄ±: {e}")


def save_evaluation_report(results, tag_performance, model):
    """
    DetaylÄ± deÄŸerlendirme raporu oluÅŸturur ve kaydeder.
    
    Args:
        results (dict): DeÄŸerlendirme sonuÃ§larÄ±
        tag_performance (dict): Tag bazlÄ± performans
        model: HMM model
    """
    print(f"\nğŸ“„ DeÄŸerlendirme raporu oluÅŸturuluyor: {EVALUATION_REPORT}")
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    with open(EVALUATION_REPORT, 'w', encoding='utf-8') as f:
        f.write("HMM POS Tagger - Evaluation Report\n")
        f.write("=" * 60 + "\n")
        f.write(f"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # Model bilgileri
        f.write("Model Information:\n")
        f.write("-" * 30 + "\n")
        f.write(f"Model File: {MODEL_FILE}\n")
        f.write(f"Test File: {TEST_FILE}\n")
        
        model_stats = model.get_model_statistics()
        f.write(f"Vocabulary Size: {model_stats.get('vocabulary_size', 0):,}\n")
        f.write(f"Tag Set Size: {model_stats.get('tag_set_size', 0):,}\n")
        f.write(f"Training Date: {model.metadata.get('training_date', 'N/A')}\n\n")
        
        # Genel performans
        f.write("Overall Performance:\n")
        f.write("-" * 30 + "\n")
        f.write(f"Sentences Processed: {results['sentences_processed']:,}\n")
        f.write(f"Total Tokens: {results['total_tokens']:,}\n")
        f.write(f"Correct Predictions: {results['correct_predictions']:,}\n")
        f.write(f"Overall Accuracy: {results['overall_accuracy']:.2f}%\n")
        f.write(f"Total Time: {results['total_time']:.2f} seconds\n")
        f.write(f"Average Sentence Time: {results['avg_sentence_time']:.3f} seconds\n")
        f.write(f"Tokens per Second: {results['tokens_per_second']:.1f}\n\n")
        
        # Tag bazlÄ± performans
        f.write("Per-Tag Performance:\n")
        f.write("-" * 30 + "\n")
        f.write(f"{'Tag':10s} {'Total':8s} {'Correct':8s} {'Acc%':8s} {'Prec%':8s} {'Rec%':8s} {'F1':8s}\n")
        f.write("-" * 70 + "\n")
        
        for tag in sorted(tag_performance.keys()):
            perf = tag_performance[tag]
            f.write(f"{tag:10s} {perf['total']:8d} {perf['correct']:8d} "
                   f"{perf['accuracy']:7.2f} {perf['precision']:7.2f} "
                   f"{perf['recall']:7.2f} {perf['f1']:7.2f}\n")
        
        # En iyi ve en kÃ¶tÃ¼ taglar
        f.write("\nBest Performing Tags (by F1 score):\n")
        best_tags = sorted(tag_performance.items(), 
                          key=lambda x: x[1]['f1'], reverse=True)[:5]
        for tag, perf in best_tags:
            f.write(f"  {tag:10s}: F1={perf['f1']:5.2f} (Acc={perf['accuracy']:5.2f}%)\n")
        
        f.write("\nWorst Performing Tags (by F1 score):\n")
        worst_tags = sorted(tag_performance.items(), 
                           key=lambda x: x[1]['f1'])[:5]
        for tag, perf in worst_tags:
            f.write(f"  {tag:10s}: F1={perf['f1']:5.2f} (Acc={perf['accuracy']:5.2f}%)\n")
        
        # Hata Ã¶rnekleri
        if DETAILED_ERRORS and results['error_examples']:
            f.write("\nError Examples (First 20):\n")
            f.write("-" * 40 + "\n")
            for i, error in enumerate(results['error_examples'][:20], 1):
                f.write(f"{i:2d}. Word: '{error['word']}' "
                       f"True: {error['true_tag']} â†’ Pred: {error['predicted_tag']}\n")
                f.write(f"    Context: {error['context'][:100]}...\n\n")
    
    print(f"âœ… DeÄŸerlendirme raporu kaydedildi")


def save_results_json(results, tag_performance):
    """
    SonuÃ§larÄ± JSON formatÄ±nda kaydeder.
    
    Args:
        results (dict): DeÄŸerlendirme sonuÃ§larÄ±
        tag_performance (dict): Tag bazlÄ± performans
    """
    print(f"ğŸ’¾ SonuÃ§lar JSON olarak kaydediliyor: {RESULTS_JSON}")
    
    # JSON serializable hale getir
    json_results = {
        'evaluation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'overall_performance': {
            'sentences_processed': results['sentences_processed'],
            'total_tokens': results['total_tokens'],
            'correct_predictions': results['correct_predictions'],
            'overall_accuracy': results['overall_accuracy'],
            'overall_precision': results['overall_precision'],
            'overall_recall': results['overall_recall'],
            'overall_f1': results['overall_f1'],
            'total_time': results['total_time'],
            'avg_sentence_time': results['avg_sentence_time'],
            'tokens_per_second': results['tokens_per_second']
        },
        'tag_performance': tag_performance,
        'error_count': len(results['error_examples'])
    }
    
    with open(RESULTS_JSON, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… JSON sonuÃ§larÄ± kaydedildi")


def print_summary(results, tag_performance):
    """
    Ã–zet sonuÃ§larÄ± ekrana yazdÄ±rÄ±r.
    
    Args:
        results (dict): DeÄŸerlendirme sonuÃ§larÄ±  
        tag_performance (dict): Tag bazlÄ± performans
    """
    print(f"\nğŸ“‹ DEÄERLENDIRME Ã–ZETÄ°")
    print(f"{'='*60}")
    print(f"ğŸ¯ Genel Accuracy     : %{results['overall_accuracy']:.2f}")
    print(f"âš¡ Ä°ÅŸlem HÄ±zÄ±        : {results['tokens_per_second']:.1f} token/saniye")
    print(f"ğŸ“Š Ä°ÅŸlenen Token     : {results['total_tokens']:,}")
    print(f"âœ… DoÄŸru Tahmin      : {results['correct_predictions']:,}")
    print(f"âŒ YanlÄ±ÅŸ Tahmin     : {results['total_tokens'] - results['correct_predictions']:,}")
    
    # En iyi/kÃ¶tÃ¼ taglar
    best_tag = max(tag_performance.items(), key=lambda x: x[1]['f1'])
    worst_tag = min(tag_performance.items(), key=lambda x: x[1]['f1'])
    
    print(f"\nğŸ† En Ä°yi Tag         : {best_tag[0]} (F1: {best_tag[1]['f1']:.2f})")
    print(f"âš ï¸  En Zor Tag         : {worst_tag[0]} (F1: {worst_tag[1]['f1']:.2f})")
    
    # Ã‡Ä±ktÄ± dosyalarÄ±
    print(f"\nğŸ“‚ OluÅŸturulan Dosyalar:")
    print(f"   ğŸ“„ Rapor: {EVALUATION_REPORT}")
    print(f"   ğŸ’¾ JSON: {RESULTS_JSON}")
    if PLOTTING_AVAILABLE and CREATE_PLOTS:
        print(f"   ğŸ“Š Confusion Matrix: {CONFUSION_MATRIX_PNG}")
        print(f"   ğŸ“ˆ Performans Grafikleri: {PERFORMANCE_PLOTS_PNG}")


def plot_overall_metrics(results):
    """
    Overall metrikleri (accuracy, precision, recall, F1) bar grafik olarak gÃ¶rselleÅŸtirir.
    """
    metrics = {
        'Accuracy': results['overall_accuracy'],
        'Precision': results['overall_precision'],
        'Recall': results['overall_recall'],
        'F1': results['overall_f1']
    }
    plt.figure(figsize=(10, 6))
    plt.bar(metrics.keys(), metrics.values(), color=['blue', 'green', 'red', 'purple'])
    plt.title('Overall Model Performance Metrics')
    plt.ylabel('Score')
    plt.ylim(0, 1)
    for i, v in enumerate(metrics.values()):
        plt.text(i, v + 0.01, f'{v:.2f}', ha='center')
    plt.tight_layout()
    plt.savefig('models/overall_metrics.png')
    plt.close()


def plot_tag_heatmap(tag_performance):
    """
    Tag bazlÄ± performans metriklerini (accuracy, precision, recall, F1) heatmap olarak gÃ¶rselleÅŸtirir.
    """
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    data = []
    for tag, perf in tag_performance.items():
        row = [perf[m] for m in metrics]
        data.append(row)
    data = np.array(data)
    plt.figure(figsize=(12, 8))
    sns.heatmap(data, annot=True, fmt='.2f', cmap='YlGnBu', xticklabels=metrics, yticklabels=tag_performance.keys())
    plt.title('Tag-based Performance Metrics Heatmap')
    plt.tight_layout()
    plt.savefig('models/tag_heatmap.png')
    plt.close()


def main():
    """
    Ana deÄŸerlendirme fonksiyonu.
    """
    print("ğŸ”¬ HMM Model DeÄŸerlendirmesi BaÅŸlÄ±yor")
    print("=" * 60)
    
    try:
        # 1. Model ve test verisini yÃ¼kle
        model, test_reader = load_model_and_test_data()
        
        # 2. Modeli deÄŸerlendir
        results = evaluate_model(model, test_reader)
        
        # 3. Tag bazlÄ± analiz
        tag_performance = analyze_per_tag_performance(results)
        
        # 4. GÃ¶rselleÅŸtirmeler oluÅŸtur
        if PLOTTING_AVAILABLE and CREATE_PLOTS:
            create_confusion_matrix(results)
            create_performance_plots(results, tag_performance)
        
        # 5. RaporlarÄ± kaydet
        save_evaluation_report(results, tag_performance, model)
        save_results_json(results, tag_performance)
        plot_overall_metrics(results)
        plot_tag_heatmap(tag_performance)
        
        # 6. Ã–zet yazdÄ±r
        print_summary(results, tag_performance)
        
        print(f"\nğŸ‰ DeÄŸerlendirme baÅŸarÄ±yla tamamlandÄ±!")
        print(f"Sonraki adÄ±m: python scripts/05_interactive_demo.py")
        
    except Exception as e:
        print(f"\nâŒ Hata: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main() 